{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP Workshop 4.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "XKY9pgjak9E3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "01364188-d243-44a7-d50b-a3c127eb0a25"
      },
      "source": [
        "# Importing all needed libraries.\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "import os\n",
        "from os.path import join\n",
        "import re\n",
        "from nltk.probability import FreqDist\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from functools import reduce\n",
        "from operator import add\n",
        "import numpy as np\n",
        "\n",
        "# Downlaoding the stopwords module.\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q8ihy0HOm-B_"
      },
      "source": [
        "# Getting the list of stopwords and the Porter Stemmer.\n",
        "stop_words = stopwords.words('english')\n",
        "porter = PorterStemmer()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "czz_UDAqmrZ-"
      },
      "source": [
        "# The Corpus Reader class.\n",
        "class CorpusReader:\n",
        "    def __init__(self, folder_path, stop_words = None, stemmer = None):\n",
        "        '''\n",
        "            The Corpus Reader constructor\n",
        "                :param folder_path: str\n",
        "                    The path to the folder with files.\n",
        "                :param stop_words: list\n",
        "                    The list o stopwords\n",
        "                :param stemmer: obj\n",
        "                    The stemmer to apply on the text data.\n",
        "        '''\n",
        "        # Setting up the parameters.\n",
        "        self.__folder_path = folder_path\n",
        "        self.__stop_words = stop_words\n",
        "        self.stemmer = stemmer\n",
        "        self.classes_ = os.listdir(self.__folder_path)\n",
        "        self.cls_dict = {}\n",
        "        self.__named_entities = set()\n",
        "        \n",
        "        # Building up the \n",
        "        for cls in self.classes_:\n",
        "            self.cls_dict[cls] = []\n",
        "            for file_path in os.listdir(join(self.__folder_path, cls)):\n",
        "                self.cls_dict[cls].append(join(self.__folder_path, cls, file_path))\n",
        "    \n",
        "    def __getitem__(self, cls):\n",
        "        '''\n",
        "            This function allows accessing the textes in the corpus by name of the class\n",
        "            and the index of the file.\n",
        "                :param cls: tuple\n",
        "                    This argument is passes as a tuple with 2 values:\n",
        "                        1. Name of the class.\n",
        "                        2. Index of the file.\n",
        "        '''\n",
        "        return open(self.cls_dict[cls[0]][cls[1]], 'r', encoding='utf-8').read()\n",
        "    \n",
        "    def __normalize(self, text):\n",
        "        '''\n",
        "            This function normalizes the text.\n",
        "                :param text: str\n",
        "                    The text that should be normalized.\n",
        "        '''\n",
        "        # Leplacing the new line symbols wiht space.\n",
        "        text = text.replace('\\n', ' ')\n",
        "        \n",
        "        # Bringing the text to the lower case.\n",
        "        text = text.lower()\n",
        "        \n",
        "        # Extracting only the words from the text.\n",
        "        text = ' '.join(re.findall('[a-z]+', text))\n",
        "        return text\n",
        "    \n",
        "    def __eliminate_stopwords(self, text):\n",
        "        '''\n",
        "            This function is deleting the stopwords from text.\n",
        "                :param text: str\n",
        "                    The text from which we shoul remove the stopwords.\n",
        "        '''\n",
        "        if self.__stop_words is not None:\n",
        "            return ' '.join([word for word in word_tokenize(text)\n",
        "                            if word not in self.__stop_words])\n",
        "    \n",
        "    def __get_hapaxes(self, dictionary):\n",
        "        '''\n",
        "            This function is responsible for finding the hapaxes.\n",
        "                :param dictionary: dict\n",
        "                    The dixtionary with the all texts separated by classes in the following\n",
        "                    structure:\n",
        "                        {<class> : [<list of textes for this class>]}\n",
        "        '''\n",
        "        # Creatting an empty dictionary for storryng the hapaxes for every class.\n",
        "        self.__hapaxes_per_classes = {}\n",
        "        \n",
        "        # Creatting an empty dictionary for storryn the frequency distribution of words\n",
        "        # for every class\n",
        "        fdist = {}\n",
        "        \n",
        "        # Iterating throw every class.\n",
        "        for cls in self.classes_:\n",
        "            fdist[cls] = FreqDist()\n",
        "            \n",
        "            # Updating the frequenncy distribution of words for the class.\n",
        "            for i in range(len(dictionary[cls])):\n",
        "                fdist[cls].update(FreqDist(word for word in word_tokenize(dictionary[cls][i])))\n",
        "            \n",
        "            # Adding the hapaxes for every class.\n",
        "            self.__hapaxes_per_classes[cls] = fdist[cls].hapaxes()\n",
        "    \n",
        "    def __stem(self, text):\n",
        "        '''\n",
        "            This function amply yhe stemmer on every word in the sentence.\n",
        "                :param text: str\n",
        "                    The text that should be stemmed.\n",
        "        '''\n",
        "        return ' '.join([self.stemmer.stem(word) for word in word_tokenize(text)])\n",
        "    \n",
        "    def __named_identity_extraction(self, text):\n",
        "        '''\n",
        "            This function finds out all named entities in a text and adds it in a set.\n",
        "                :param text: str\n",
        "                    The text from which we should extract the named entities.\n",
        "        '''\n",
        "        for sent in sent_tokenize(text):\n",
        "            for chunk in nltk.ne_chunk(nltk.pos_tag(word_tokenize(sent))):\n",
        "                if hasattr(chunk, 'label'):\n",
        "                    self.__named_entities.add(' '.join(c[0] for c in chunk))\n",
        "    \n",
        "    def process(self):\n",
        "        '''\n",
        "            This function is setting up the corpus reader based on the corpus sent in the \n",
        "            constructor of the reader.\n",
        "        '''\n",
        "        # Creatting a dictionary with all texted from the corpus separated by classes.\n",
        "        text_data = {cls : [] for cls in self.classes_}\n",
        "        for cls in self.classes_:\n",
        "            for i in range(len(os.listdir(join(self.__folder_path, cls)))):\n",
        "                # Loading the corpus in the dictionary.\n",
        "                text_data[cls].append(self.__normalize(self[cls, i]))\n",
        "                \n",
        "                # Adding the names entities extracted for mthe text.\n",
        "                self.__named_identity_extraction(text_data[cls][-1])\n",
        "        \n",
        "        # Gathering all named entities in a general list.\n",
        "        self.named_entities = list(self.__named_entities)\n",
        "        \n",
        "        # Replacing the spaces in named entities wiht underscores.\n",
        "        self.named_entities = [ni.replace(' ', '_') for ni in self.named_entities\n",
        "                              if len(ni) >= 2]\n",
        "        \n",
        "        # Itereting throw every text in the corpus and replacing spaces in named entities\n",
        "        # with underscores.\n",
        "        for cls in self.classes_:\n",
        "            for i in range(len(text_data[cls])):\n",
        "                for ni in self.named_entities:\n",
        "                    text_data[cls][i] = text_data[cls][i].replace(' '.join(\n",
        "                        ni.split('_')), ni)\n",
        "        \n",
        "        # Getting the hapaxes.\n",
        "        self.__get_hapaxes(text_data)\n",
        "        self.common_hapaxes = list(reduce(add, [self.__hapaxes_per_classes[cls]\n",
        "                                               for cls in self.classes_]))\n",
        "        \n",
        "        # Applying the last preprocessing on the text.\n",
        "        for cls in self.classes_:\n",
        "            for i in range(len(text_data[cls])):\n",
        "                # Eliminating the hapaxes.\n",
        "                text_data[cls][i] = ' '.join([word for word in word_tokenize(text_data[cls][i])\n",
        "                                             if word not in self.common_hapaxes])\n",
        "                \n",
        "                # Eliminating the stopwords.\n",
        "                text_data[cls][i] = self.__eliminate_stopwords(text_data[cls][i])\n",
        "                \n",
        "                # Stemming the text.\n",
        "                text_data[cls][i] = self.__stem(text_data[cls][i])\n",
        "                \n",
        "                # Eliminating words with fewer thant 3 letters.\n",
        "                text_data[cls][i] = ' '.join([word for word in word_tokenize(text_data[cls][i])\n",
        "                                             if len(word) >= 3])\n",
        "        \n",
        "        # Generating the X matrix and the y vector for the Machine Learning Pipeline.\n",
        "        X = np.array([text_data[cls][i] for cls in self.classes_ \n",
        "                      for i in range(len(text_data[cls]))])\n",
        "        y = np.array([cls for cls in self.classes_ for i in range(len(text_data[cls]))])\n",
        "        return X, y\n",
        "    \n",
        "    def apply(self, path):\n",
        "        # Creatting a dictionary with all texted from the corpus separated by classes.\n",
        "        text_data = {cls : [] for cls in self.classes_}\n",
        "        \n",
        "        for cls in self.classes_:\n",
        "            # Loading the corpus in the dictionary.\n",
        "            for file_path in os.listdir(join(path, cls)):\n",
        "                text_data[cls].append(self.__normalize(open(\n",
        "                    join(path, cls, file_path), 'r', encoding='utf-8'\n",
        "                ).read()))\n",
        "        \n",
        "        # Itereting throw every text in the corpus and replacing spaces in named entities\n",
        "        # with underscores.\n",
        "        for cls in self.classes_:\n",
        "            for i in range(len(text_data[cls])):\n",
        "                for ni in self.named_entities:\n",
        "                    text_data[cls][i] = text_data[cls][i].replace(' '.join(\n",
        "                        ni.split('_')), ni)\n",
        "        \n",
        "        # Applying the last preprocessing on the text.\n",
        "        for cls in self.classes_:\n",
        "            for i in range(len(text_data[cls])):\n",
        "                # Eliminating the hapaxes.\n",
        "                text_data[cls][i] = ' '.join([word for word in word_tokenize(text_data[cls][i])\n",
        "                                             if word not in self.common_hapaxes])\n",
        "                \n",
        "                # Eliminating the stopwords.\n",
        "                text_data[cls][i] = self.__eliminate_stopwords(text_data[cls][i])\n",
        "                \n",
        "                # Stemming the text.\n",
        "                text_data[cls][i] = self.__stem(text_data[cls][i])\n",
        "                \n",
        "                # Eliminating words with fewer thant 3 letters.\n",
        "                text_data[cls][i] = ' '.join([word for word in word_tokenize(text_data[cls][i])\n",
        "                                             if len(word) >= 3])\n",
        "        \n",
        "        # Generating the X matrix and the y vector for the Machine Learning Pipeline.\n",
        "        X = np.array([text_data[cls][i] for cls in self.classes_ \n",
        "                      for i in range(len(text_data[cls]))])\n",
        "        y = np.array([cls for cls in self.classes_ for i in range(len(text_data[cls]))])\n",
        "        return X, y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 299
        },
        "id": "VnuGQU9BHfRK",
        "outputId": "2b9d8938-cf1b-4613-d9db-99374c666cfe"
      },
      "source": [
        "# Creating the corpus reader.\n",
        "reader = CorpusReader(r'D:\\NLP\\BIO_CS_DATA\\TRAIN', stop_words, porter)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-20ff8bc31b03>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Creatting the corpus reader.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mreader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCorpusReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr'D:\\NLP\\BIO_CS_DATA\\TRAIN'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mporter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-9-40551c1869c0>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, folder_path, stop_words, stemmer)\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__stop_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstop_words\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstemmer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstemmer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__folder_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcls_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__named_entities\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'D:\\\\NLP\\\\BIO_CS_DATA\\\\TRAIN'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sWyOsyxUIQC1"
      },
      "source": [
        "reader = ['biology', 0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UieJrU61IbYG"
      },
      "source": [
        "# processing the training corpus.\n",
        "X_train, y_train = reader.process()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QR4GAkgGId3J"
      },
      "source": [
        "# Applying the changes on the test corpus.\n",
        "X_test, y_test = reader.apply(r'D:\\NLP\\BIO_CS_DATA\\TEST')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TeZfFx7CJj4A"
      },
      "source": [
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "crQ1qFBFJpuY"
      },
      "source": [
        "train_df = pd.DataFrame({'text' : X_train, 'class' : y_train})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9EvIAumYKHTR"
      },
      "source": [
        "train_df.to_csv('train.csv', sep = '\\t', index = False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EObzSX10KY8u"
      },
      "source": [
        "test_df = pd.DataFrame({'text' : X_test, 'class' : y_test})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gJfqRAOa4dPM"
      },
      "source": [
        "# Homework"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qVRsDDWd4gMx"
      },
      "source": [
        "import numpy as np\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from sklearn.utils.validation import check_X_y, check_array, check_is_fitted\n",
        "from sklearn.utils.multiclass import unique_labels\n",
        "from sklearn.metrics import euclidean_distances\n"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1CHor47I4kiG"
      },
      "source": [
        "class TextNormalizer(BaseEstimator, TransformerMixin):\n",
        "    def fit(self, X, y):\n",
        "          return self\n",
        "    def transform(self, X):\n",
        "          for i in range(len(X)):\n",
        "              X[i] = X[i].replace('\\n', ' ')\n",
        "              X[i] = X[i].replace('\\r', ' ')\n",
        "              X[i] = X[i].lower()\n",
        "          return X"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "whXl_117QVTF",
        "outputId": "f454284f-9baf-47fb-b2c4-ab898d900d30"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gPOlBvfa-7Gx"
      },
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "class WordsExtractor(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, stopwords_list):\n",
        "      self.__stopwords_list = stopwords_list\n",
        "    def fit(self, X, y):\n",
        "      self.__hapaxes = {}\n",
        "      self.__fdist = {}\n",
        "      for i in range(len(X)):\n",
        "        self.__fdist.update(FreqDist(word_tokenize(X[i])))\n",
        "      self.__hapaxes = self.__fidist.hapaxes()\n",
        "    def transform(self, X, y):\n",
        "      NewX = []\n",
        "      for i in range(len(X)):\n",
        "        Newstring = ''\n",
        "        for word in word_tokenize(X[i]):\n",
        "           if word not in self.__hapaxes and word not in self.stopwords_list:\n",
        "                Newstring += word + ' '\n",
        "        NewX.append(Newstring)\n",
        "      return np.array(NewX)\n",
        " # sau\n",
        " #     X[i] = ' '.join([word for word in word_tokenize(X[i]) if word if not in self.__hapaxes and word not in self.stopwords_list])"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bIJonWXrEhtU"
      },
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "class ApplyStemmer(BaseEstimator, TransformerMixin):\n",
        "  def __init__(self, stemmer):\n",
        "    self.__stemmer = stemmer\n",
        "  def fit(self, X, y):\n",
        "    return self\n",
        "  def transform(self, X, y):\n",
        "    NewX = []\n",
        "    for i in range(len(X)):\n",
        "      NewX = ''\n",
        "      for word in word_tokenize(X[i]):\n",
        "        X[i] = ' '.join(self.__stemmer.stem(word))\n",
        "      NewX.append(NewString)\n",
        "    return np.array(NewX)\n",
        "# sau\n",
        "#  X[i] = ' '.join([self.__stemmer.stem(word) for word in word_tokenize(X[i])])"
      ],
      "execution_count": 15,
      "outputs": []
    }
  ]
}
